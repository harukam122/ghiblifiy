{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harukam122/ghiblifiy/blob/main/ghiblify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLKjcm91UnLd"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-7PzGAP4PQR",
        "outputId": "2e4be0c9-c627-4a38-e597-c4680b6ba538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrCMIitR9Vu_"
      },
      "outputs": [],
      "source": [
        "# cd drive/MyDrive/DL\\ Final\\ Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK2vETozCS4t",
        "outputId": "e6a83fe4-2c6d-4308-c741-e7af52a91f4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/12unBA6eflIDir_xYDi0aG-jbdDiDCrqf/DL Final Project\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/.shortcut-targets-by-id/12unBA6eflIDir_xYDi0aG-jbdDiDCrqf/DL Final Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQzy7SdsAFHt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n",
        "\n",
        "import IPython.display as display\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12, 12)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import time\n",
        "import functools\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "device=torch.device(\"cuda\" if (torch.cuda.is_available()) else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAIWUwnuPImK"
      },
      "outputs": [],
      "source": [
        "# content_url = \"/content/drive/.shortcut-targets-by-id/12unBA6eflIDir_xYDi0aG-jbdDiDCrqf/DL Final Project/images/content_three.jpeg\"\n",
        "# style_url = \"/content/drive/.shortcut-targets-by-id/12unBA6eflIDir_xYDi0aG-jbdDiDCrqf/DL Final Project/images/style_three.jpeg\"\n",
        "style_url = \"/content/drive/Shared-with-me/DL Final Project/images/style_two.jpg\"\n",
        "content_url = \"/content/drive/Shared-with-me/DL Final Project/images/content_two.jpg\"\n",
        "# style_url = \"/content/drive/Shared with me/DL Final Project/images/style_two.jpg\"\n",
        "# content_url = \"/content/drive/Shared with me/DL Final Project/images/content_two.jpeg\"\n",
        "\n",
        "# content_url = \"/content/drive/MyDrive/DL Final Project/images/labrador_content.jpg\"\n",
        "# style_url = \"/content/drive/MyDrive/DL Final Project/images/kandinsky_style.jpg\"\n",
        "\n",
        "# Define a function for normalizing the images\n",
        "def normalize(img):\n",
        "    return transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(img)\n",
        "\n",
        "# Define a function for denormalizing the images\n",
        "def denormalize(img):\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).cuda()\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).cuda()\n",
        "    return img * std[:, None, None] + mean[:, None, None]\n",
        "\n",
        "def load_img(path_to_img):\n",
        "    max_dim = 512\n",
        "    img = Image.open(path_to_img).convert('RGB')\n",
        "    width, height = img.size\n",
        "    if height > width:\n",
        "        new_height = max_dim\n",
        "        new_width = int(max_dim * width / height)\n",
        "    else:\n",
        "        new_width = max_dim\n",
        "        new_height = int(max_dim * height / width)\n",
        "    img = transforms.Resize((new_height, new_width))(img)\n",
        "    img = transforms.ToTensor()(img)\n",
        "    img = normalize(img)\n",
        "    img = img.unsqueeze(0) # add batch dimension\n",
        "    return img.to(device,torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UttvDiKFPM32"
      },
      "outputs": [],
      "source": [
        "def imshow(image, title=None):\n",
        "  image = image.cuda().clone()\n",
        "  if len(image.shape) > 3:\n",
        "    image = image.squeeze(0)\n",
        "  image = denormalize(image)\n",
        "  image = image.detach().cpu().numpy()\n",
        "  image = image.squeeze().transpose(1, 2, 0)\n",
        "  image = image.clip(0,1)\n",
        "  if title:\n",
        "    plt.imshow(image, title)\n",
        "  else:\n",
        "    plt.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "cWwcS3dvPRfX",
        "outputId": "ca2f0a07-f93a-4f36-8ed1-3522e591f0c4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a58dc8a3abfa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontent_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstyle_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-ca2e4ed30d7f>\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path_to_img)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmax_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2975\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2976\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/Shared-with-me/DL Final Project/images/content_two.jpg'"
          ]
        }
      ],
      "source": [
        "content_image = load_img(content_url)\n",
        "style_image = load_img(style_url)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "imshow(content_image)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "imshow(style_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtIzCqSrVEQA"
      },
      "source": [
        "# Defining Content/Style Representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDm0m88wFBHo"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "\n",
        "content_layers = ['31']\n",
        "style_layers = ['1', '6', '11', '20', '29']\n",
        "\n",
        "# content_layers = {\n",
        "#     '30': 'block5_conv2'\n",
        "#   }\n",
        "\n",
        "# style_layers = {\n",
        "#     '0': 'block1_conv1',\n",
        "#     '5': 'block2_conv1',\n",
        "#     '10': 'block3_conv1',\n",
        "#     '19': 'block4_conv1',\n",
        "#     '28': 'block5_conv1',\n",
        "#   }\n",
        "\n",
        "num_content_layers = len(content_layers)\n",
        "num_style_layers = len(style_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGsf8rcLz2iX"
      },
      "source": [
        "# Gram Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Zl69E_oK9AC"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(features):\n",
        "    # first dimension is batch dimension\n",
        "    _, c, h, w = features.size()\n",
        "    features = features.view(c, h*w)\n",
        "    return features@features.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiezUlG3z9Ib"
      },
      "source": [
        "# Extract style and content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki4wNk2sMR2M"
      },
      "outputs": [],
      "source": [
        "class StyleContentModel(torch.nn.Module):\n",
        "  def __init__(self, style_layers, content_layers):\n",
        "    super(StyleContentModel, self).__init__()\n",
        "    self.vgg = models.vgg19(pretrained=True).features.cuda().eval()\n",
        "    for param in self.vgg.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    self.layers = style_layers + content_layers\n",
        "    self.extractor = create_feature_extractor(self.vgg, return_nodes=self.layers)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    feat_dict = self.extractor(inputs)\n",
        "    features = list(feat_dict.values())\n",
        "    \n",
        "    # Sort layer outputs to style and content layers\n",
        "    style_outputs, content_outputs = (features[:num_style_layers],\n",
        "                                      features[num_style_layers:])\n",
        "  \n",
        "\n",
        "    # apply gram matrix\n",
        "    style_outputs = [gram_matrix(output) for output in style_outputs]\n",
        "\n",
        "    # Assign content and style representations as dictionaries\n",
        "    content_dict = {content_name: value\n",
        "                    for content_name, value\n",
        "                    in zip(content_layers, content_outputs)}\n",
        "    style_dict = {style_name: value\n",
        "                  for style_name, value\n",
        "                  in zip(style_layers, style_outputs)}\n",
        "\n",
        "    return {'content': content_dict, 'style': style_dict}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO6Wk1etkSAX"
      },
      "outputs": [],
      "source": [
        "extractor = StyleContentModel(style_layers, content_layers)\n",
        "results = extractor(content_image)\n",
        "\n",
        "print('Styles:')\n",
        "for name, output in sorted(results['style'].items()):\n",
        "    print(\"  \", name)\n",
        "    print(\"    shape: \", output.cpu().detach().numpy().shape)\n",
        "    print(\"    min: \", output.cpu().detach().numpy().min())\n",
        "    print(\"    max: \", output.cpu().detach().numpy().max())\n",
        "    print(\"    mean: \", output.cpu().detach().numpy().mean())\n",
        "    print()\n",
        "\n",
        "print(\"Contents:\")\n",
        "for name, output in sorted(results['content'].items()):\n",
        "    print(\"  \", name)\n",
        "    # if output.shape[1] > 1:\n",
        "    #     output = output[:, -2:-1, :, :]\n",
        "    # imshow(output[0], title=name)\n",
        "    print(\"    shape: \", output.cpu().detach().numpy().shape)\n",
        "    print(\"    min: \", output.cpu().detach().numpy().min())\n",
        "    print(\"    max: \", output.cpu().detach().numpy().max())\n",
        "    print(\"    mean: \", output.cpu().detach().numpy().mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbKo51OUt0tE"
      },
      "source": [
        "# Run Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YF5aW9FHt6eQ"
      },
      "outputs": [],
      "source": [
        "style_targets = extractor(style_image)['style']\n",
        "content_targets = extractor(content_image)['content']\n",
        "\n",
        "image = content_image.clone().requires_grad_(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJ4Uh1cquY8N"
      },
      "outputs": [],
      "source": [
        "def clip_0_1(image):\n",
        "    return torch.clamp(image, min=0.0, max=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gibv3FR2unyF"
      },
      "outputs": [],
      "source": [
        "opt = torch.optim.Adam([image], lr=0.02, betas=(0.99, 0.999), eps=1e-1)\n",
        "\n",
        "# Weighted combination of losses\n",
        "style_weight=1e-2\n",
        "content_weight=1e4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1urfe6exwVt5"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def style_content_loss(outputs):\n",
        "    style_outputs = outputs['style']\n",
        "    content_outputs = outputs['content']\n",
        "\n",
        "    # Mean squared error for every GRAM MATRIX of style layer outputs\n",
        "    style_loss = sum([F.mse_loss(style_outputs[name], style_targets[name]) \n",
        "                      for name in style_outputs.keys()])\n",
        "    style_loss *= style_weight / num_style_layers\n",
        "\n",
        "    # Mean squared error for every content layer outputs\n",
        "    content_loss = sum([F.mse_loss(content_outputs[name], content_targets[name]) \n",
        "                        for name in content_outputs.keys()])\n",
        "    content_loss *= content_weight / num_content_layers\n",
        "\n",
        "    # Sum losses\n",
        "    loss = style_loss + content_loss\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp57y4bFwhrj"
      },
      "outputs": [],
      "source": [
        "def train_step(image):\n",
        "    outputs = extractor(image)\n",
        "    loss = style_content_loss(outputs)\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    # Clip the image pixel values to be between 0 and 1\n",
        "    clip_0_1(image)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY9_Eq3_wuri"
      },
      "outputs": [],
      "source": [
        "train_step(image)\n",
        "train_step(image)\n",
        "train_step(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obqGi1qVKxAg"
      },
      "outputs": [],
      "source": [
        "# display content and final, target image\n",
        "imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fCK5M-5MQvf"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "epochs = 10\n",
        "steps_per_epoch = 100\n",
        "\n",
        "step = 0\n",
        "for n in range(epochs):\n",
        "    for m in range(steps_per_epoch):\n",
        "        step += 1\n",
        "        loss = train_step(image)\n",
        "        print(\".\", end='', flush=True)\n",
        "    imshow(image)\n",
        "    plt.show()\n",
        "    print(\"Train step: {}\".format(step) + \" Loss: {}\".format(loss))\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total time: {:.1f}\".format(end-start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcFU2rSwzTNN"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "def transfer_style(gen_image, content_image, style_image, epochs = 10, steps_per_epoch = 100):\n",
        "  opt = torch.optim.Adam([image], lr=learning_rate, betas=(0.99, 0.999), eps=1e-1)\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  style_targets = extractor(style_image)['style']\n",
        "  content_targets = extractor(content_image)['content']\n",
        "\n",
        "  step = 0\n",
        "  for n in range(epochs):\n",
        "      for m in range(steps_per_epoch):\n",
        "          step += 1\n",
        "          loss = train_step(gen_img)\n",
        "          print(\".\", end='', flush=True)\n",
        "      imshow(gen_img)\n",
        "      plt.show()\n",
        "      print(\"Train step: {}\".format(step) + \" Loss: {}\".format(loss))\n",
        "\n",
        "  end = time.time()\n",
        "  print(\"Total time: {:.1f}\".format(end-start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOZJ4ucJnSsr"
      },
      "outputs": [],
      "source": [
        "# HYPER PARAMETERS\n",
        "learning_rate = 0.02\n",
        "epochs = 10\n",
        "steps_per_epoch = 100\n",
        "style_weight=1e-2\n",
        "content_weight=1e4\n",
        "\n",
        "# INPUTS\n",
        "# content_url = \"/content/drive/MyDrive/DL Final Project/images/content_img.jpg\"\n",
        "# style_url = \"/content/drive/MyDrive/DL Final Project/images/ghibli_style.jpg\"\n",
        "content_url = \"/content/drive/.shortcut-targets-by-id/12unBA6eflIDir_xYDi0aG-jbdDiDCrqf/DL Final Project/images/content_three.jpeg\"\n",
        "style_url = \"/content/drive/.shortcut-targets-by-id/12unBA6eflIDir_xYDi0aG-jbdDiDCrqf/DL Final Project/images/style_three.jpeg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhBvBjz4oR6f"
      },
      "outputs": [],
      "source": [
        "content_image = load_img(content_url)\n",
        "style_image = load_img(style_url)\n",
        "gen_img = content_image.clone().requires_grad_(True)\n",
        "\n",
        "# DOESN'T WORK FOR SM REASN\n",
        "transfer_style(gen_img, content_image, style_image, epochs, steps_per_epoch)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}